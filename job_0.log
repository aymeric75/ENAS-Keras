2021-07-14 13:34:35.169934: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcudart.so.11.0
2021-07-14 13:34:40.883032: I tensorflow/stream_executor/platform/default/dso_loader.cc:53] Successfully opened dynamic library libcuda.so.1
2021-07-14 13:34:40.930779: E tensorflow/stream_executor/cuda/cuda_driver.cc:328] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected
2021-07-14 13:34:40.930870: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:169] retrieving CUDA diagnostic information for host: r23g37
2021-07-14 13:34:40.930892: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:176] hostname: r23g37
2021-07-14 13:34:40.931023: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:200] libcuda reported version is: 460.32.3
2021-07-14 13:34:40.931099: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:204] kernel reported version is: 460.32.3
2021-07-14 13:34:40.931114: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:310] kernel version seems to match DSO: 460.32.3
2021-07-14 13:34:40.932110: I tensorflow/core/platform/cpu_feature_guard.cc:142] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F FMA
To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.
2021-07-14 13:34:40.959083: I tensorflow/core/distributed_runtime/rpc/grpc_channel.cc:301] Initialize GrpcChannelCache for job worker -> {0 -> localhost:12345, 1 -> localhost:23456}
2021-07-14 13:34:40.963556: I tensorflow/core/distributed_runtime/rpc/grpc_server_lib.cc:411] Started server with target: grpc://localhost:12345
r23g37:65510:66163 [10936] NCCL INFO Bootstrap : Using ib0:172.23.6.171<0>
r23g37:65510:66163 [0] NCCL INFO NET/Plugin : No plugin found (libnccl-net.so), using internal implementation
r23g37:65510:66163 [805317304] NCCL INFO NET/IB : Using [0]mlx5_1:1/IB [1]mlx5_0:1/IB ; OOB ib0:172.23.6.171<0>
r23g37:65510:66163 [0] NCCL INFO Using network IB
2021-07-14 13:35:05.568475: E tensorflow/core/common_runtime/base_collective_executor.cc:243] BaseCollectiveExecutor::StartAbort Invalid argument: Shape mismatch in the collective instance 111. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [12000] but another member in the group expected shape [2,5,10,10]. This is likely due to different input shapes at different members of the collective op.
num_replicas_in_sync : 2
number of replicas 2 : 2
Traceback (most recent call last):
  File "main.py", line 110, in <module>
    main()
  File "main.py", line 89, in main
    controller.compute_accuracies(5, 5, strategy, global_batch_size, print_file=0)
  File "/vsc-hard-mounts/leuven-data/339/vsc33965/NAS_Data/final_thesis/NAS/ENAS/ENAS-total/./Controller.py", line 784, in compute_accuracies
    model = self.get_compiled_cnn_model(cells_array)
  File "/vsc-hard-mounts/leuven-data/339/vsc33965/NAS_Data/final_thesis/NAS/ENAS/ENAS-total/./Controller.py", line 279, in get_compiled_cnn_model
    x = layers.BatchNormalization(name="outside_batchnorm2")(x)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 969, in __call__
    return self._functional_construction_call(inputs, args, kwargs,
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 1107, in _functional_construction_call
    outputs = self._keras_tensor_symbolic_call(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 840, in _keras_tensor_symbolic_call
    return self._infer_output_signature(inputs, args, kwargs, input_masks)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 878, in _infer_output_signature
    self._maybe_build(inputs)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 2625, in _maybe_build
    self.build(input_shapes)  # pylint:disable=not-callable
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/keras/layers/normalization.py", line 405, in build
    self.gamma = self.add_weight(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer.py", line 639, in add_weight
    variable = self._add_variable_with_custom_getter(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/training/tracking/base.py", line 810, in _add_variable_with_custom_getter
    new_variable = getter(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/keras/engine/base_layer_utils.py", line 127, in make_variable
    return tf_variables.VariableV1(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/variables.py", line 260, in __call__
    return cls._variable_v1_call(*args, **kwargs)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/variables.py", line 206, in _variable_v1_call
    return previous_getter(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/variables.py", line 67, in getter
    return captured_getter(captured_previous, **kwargs)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_lib.py", line 2111, in creator_with_resource_vars
    created = self._create_variable(next_creator, **kwargs)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py", line 535, in _create_variable
    return distribute_utils.create_mirrored_variable(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/distribute/distribute_utils.py", line 306, in create_mirrored_variable
    value_list = real_mirrored_creator(**kwargs)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/distribute/mirrored_strategy.py", line 530, in _real_mirrored_creator
    v = next_creator(**kwargs)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/variables.py", line 199, in <lambda>
    previous_getter = lambda **kwargs: default_variable_creator(None, **kwargs)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/variable_scope.py", line 2612, in default_variable_creator
    return resource_variable_ops.ResourceVariable(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/variables.py", line 264, in __call__
    return super(VariableMetaclass, cls).__call__(*args, **kwargs)
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 1584, in __init__
    self._init_from_args(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/resource_variable_ops.py", line 1722, in _init_from_args
    initial_value = initial_value()
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/distribute/collective_all_reduce_strategy.py", line 570, in initial_value_fn
    bcast_send = collective_ops.broadcast_send(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/collective_ops.py", line 254, in broadcast_send
    return gen_collective_ops.collective_bcast_send(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/gen_collective_ops.py", line 242, in collective_bcast_send
    return collective_bcast_send_eager_fallback(
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/ops/gen_collective_ops.py", line 300, in collective_bcast_send_eager_fallback
    _result = _execute.execute(b"CollectiveBcastSend", 1, inputs=_inputs_flat,
  File "/data/leuven/339/vsc33965/miniconda3/envs/PythonGPU/lib/python3.8/site-packages/tensorflow/python/eager/execute.py", line 59, in quick_execute
    tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,
tensorflow.python.framework.errors_impl.InvalidArgumentError: [_Derived_]Collective ops is aborted by: Shape mismatch in the collective instance 111. Op at device /job:worker/replica:0/task:0/device:CPU:0 expected shape [12000] but another member in the group expected shape [2,5,10,10]. This is likely due to different input shapes at different members of the collective op.
The error could be from a previous operation. Restart your program to reset. [Op:CollectiveBcastSend]
